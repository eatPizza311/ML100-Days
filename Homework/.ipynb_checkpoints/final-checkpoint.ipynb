{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2823 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_path = './final_data/train/'\n",
    "test_path = './final_data/test/'\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rotation_range=40,\n",
    "                                   rescale = 1./255,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,)\n",
    "#                                    validation_split=0.1)\n",
    "\n",
    "train_batches = train_datagen.flow_from_directory(train_path,\n",
    "                                                  interpolation='bicubic',\n",
    "                                                  target_size=IMAGE_SIZE,\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=True,\n",
    "                                                  subset='training')\n",
    "\n",
    "validation_batches = train_datagen.flow_from_directory(train_path,\n",
    "                                                  interpolation='bicubic',\n",
    "                                                  target_size=IMAGE_SIZE,\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=False,\n",
    "                                                  subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip': 4}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_checkpointer = ModelCheckpoint(filepath = './best.hdf5',\n",
    "                                  monitor = 'acc',\n",
    "                                  save_best_only = True,\n",
    "                                  mode = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ChihYing\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChihYing\\Anaconda3\\lib\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ChihYing\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\ChihYing\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/200\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 2.6953 - acc: 0.3585\n",
      "Epoch 2/200\n",
      "100/100 [==============================] - 76s 757ms/step - loss: 1.6394 - acc: 0.5612\n",
      "Epoch 3/200\n",
      "100/100 [==============================] - 76s 763ms/step - loss: 1.2500 - acc: 0.6590\n",
      "Epoch 4/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.9508 - acc: 0.7304\n",
      "Epoch 5/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.8881 - acc: 0.7654\n",
      "Epoch 6/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.6830 - acc: 0.7953\n",
      "Epoch 7/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.6958 - acc: 0.7985\n",
      "Epoch 8/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.5879 - acc: 0.8309\n",
      "Epoch 9/200\n",
      "100/100 [==============================] - 76s 763ms/step - loss: 0.6137 - acc: 0.8183\n",
      "Epoch 10/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.5057 - acc: 0.8526\n",
      "Epoch 11/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.5128 - acc: 0.8584\n",
      "Epoch 12/200\n",
      "100/100 [==============================] - 77s 765ms/step - loss: 0.4222 - acc: 0.8822\n",
      "Epoch 13/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.4187 - acc: 0.8708\n",
      "Epoch 14/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.3791 - acc: 0.8854\n",
      "Epoch 15/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.3302 - acc: 0.8936\n",
      "Epoch 16/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.3470 - acc: 0.8922\n",
      "Epoch 17/200\n",
      "100/100 [==============================] - 76s 761ms/step - loss: 0.2986 - acc: 0.9056\n",
      "Epoch 18/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.3078 - acc: 0.9058\n",
      "Epoch 19/200\n",
      "100/100 [==============================] - 77s 770ms/step - loss: 0.2555 - acc: 0.9145\n",
      "Epoch 20/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.2568 - acc: 0.9193\n",
      "Epoch 21/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.2437 - acc: 0.9201\n",
      "Epoch 22/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.2316 - acc: 0.9251\n",
      "Epoch 23/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.2047 - acc: 0.9334\n",
      "Epoch 24/200\n",
      "100/100 [==============================] - 77s 765ms/step - loss: 0.2135 - acc: 0.9316\n",
      "Epoch 25/200\n",
      "100/100 [==============================] - 76s 762ms/step - loss: 0.1749 - acc: 0.9412\n",
      "Epoch 26/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.1968 - acc: 0.9344\n",
      "Epoch 27/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.1608 - acc: 0.9451\n",
      "Epoch 28/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.1478 - acc: 0.9526\n",
      "Epoch 29/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.1578 - acc: 0.9459\n",
      "Epoch 30/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.1372 - acc: 0.9541\n",
      "Epoch 31/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.1273 - acc: 0.9555\n",
      "Epoch 32/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.1215 - acc: 0.9522\n",
      "Epoch 33/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.1097 - acc: 0.9598\n",
      "Epoch 34/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.1191 - acc: 0.9590\n",
      "Epoch 35/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.1316 - acc: 0.9583\n",
      "Epoch 36/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0782 - acc: 0.9722\n",
      "Epoch 37/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.1015 - acc: 0.9651\n",
      "Epoch 38/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0998 - acc: 0.9659\n",
      "Epoch 39/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0875 - acc: 0.9711\n",
      "Epoch 40/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0903 - acc: 0.9712\n",
      "Epoch 41/200\n",
      "100/100 [==============================] - 76s 761ms/step - loss: 0.0902 - acc: 0.9693\n",
      "Epoch 42/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0828 - acc: 0.9711\n",
      "Epoch 43/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0691 - acc: 0.9751\n",
      "Epoch 44/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0675 - acc: 0.9775\n",
      "Epoch 45/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0683 - acc: 0.9745\n",
      "Epoch 46/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0635 - acc: 0.9784\n",
      "Epoch 47/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0643 - acc: 0.9759\n",
      "Epoch 48/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0676 - acc: 0.9786\n",
      "Epoch 49/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0584 - acc: 0.9809\n",
      "Epoch 50/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.0610 - acc: 0.9761\n",
      "Epoch 51/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0757 - acc: 0.9790\n",
      "Epoch 52/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0461 - acc: 0.9828\n",
      "Epoch 53/200\n",
      "100/100 [==============================] - 77s 770ms/step - loss: 0.0508 - acc: 0.9820\n",
      "Epoch 54/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0558 - acc: 0.9831\n",
      "Epoch 55/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0461 - acc: 0.9869\n",
      "Epoch 56/200\n",
      "100/100 [==============================] - 77s 765ms/step - loss: 0.0441 - acc: 0.9851\n",
      "Epoch 57/200\n",
      "100/100 [==============================] - 76s 761ms/step - loss: 0.0384 - acc: 0.9881\n",
      "Epoch 58/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.0493 - acc: 0.9839\n",
      "Epoch 59/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.0437 - acc: 0.9887\n",
      "Epoch 60/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.0339 - acc: 0.9897\n",
      "Epoch 61/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0370 - acc: 0.9870\n",
      "Epoch 62/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.0319 - acc: 0.9900\n",
      "Epoch 63/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.0327 - acc: 0.9884\n",
      "Epoch 64/200\n",
      "100/100 [==============================] - 77s 770ms/step - loss: 0.0392 - acc: 0.9841\n",
      "Epoch 65/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0306 - acc: 0.9897\n",
      "Epoch 66/200\n",
      "100/100 [==============================] - 77s 772ms/step - loss: 0.0416 - acc: 0.9865\n",
      "Epoch 67/200\n",
      "100/100 [==============================] - 77s 770ms/step - loss: 0.0408 - acc: 0.9856\n",
      "Epoch 68/200\n",
      "100/100 [==============================] - 77s 770ms/step - loss: 0.0281 - acc: 0.9909\n",
      "Epoch 69/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.0353 - acc: 0.9883\n",
      "Epoch 70/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.0265 - acc: 0.9906\n",
      "Epoch 71/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0179 - acc: 0.9934\n",
      "Epoch 72/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0248 - acc: 0.9928\n",
      "Epoch 73/200\n",
      "100/100 [==============================] - 76s 761ms/step - loss: 0.0217 - acc: 0.9947\n",
      "Epoch 74/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0286 - acc: 0.9937\n",
      "Epoch 75/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0190 - acc: 0.9944\n",
      "Epoch 76/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0176 - acc: 0.9950\n",
      "Epoch 77/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0239 - acc: 0.9916\n",
      "Epoch 78/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0201 - acc: 0.9944\n",
      "Epoch 79/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0211 - acc: 0.9925\n",
      "Epoch 80/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0339 - acc: 0.9890\n",
      "Epoch 81/200\n",
      "100/100 [==============================] - 76s 762ms/step - loss: 0.0195 - acc: 0.9917\n",
      "Epoch 82/200\n",
      "100/100 [==============================] - 77s 771ms/step - loss: 0.0255 - acc: 0.9925\n",
      "Epoch 83/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0231 - acc: 0.9934\n",
      "Epoch 84/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0184 - acc: 0.9941\n",
      "Epoch 85/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.0138 - acc: 0.9953\n",
      "Epoch 86/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.0060 - acc: 0.9978\n",
      "Epoch 87/200\n",
      "100/100 [==============================] - 77s 770ms/step - loss: 0.0224 - acc: 0.9934\n",
      "Epoch 88/200\n",
      "100/100 [==============================] - 77s 771ms/step - loss: 0.0126 - acc: 0.9959\n",
      "Epoch 89/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.0142 - acc: 0.9950\n",
      "Epoch 90/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0151 - acc: 0.9950\n",
      "Epoch 91/200\n",
      "100/100 [==============================] - 77s 772ms/step - loss: 0.0125 - acc: 0.9936\n",
      "Epoch 92/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.0128 - acc: 0.9945\n",
      "Epoch 93/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0148 - acc: 0.9947\n",
      "Epoch 94/200\n",
      "100/100 [==============================] - 77s 770ms/step - loss: 0.0224 - acc: 0.9931\n",
      "Epoch 95/200\n",
      "100/100 [==============================] - 77s 772ms/step - loss: 0.0288 - acc: 0.9915\n",
      "Epoch 96/200\n",
      "100/100 [==============================] - 77s 774ms/step - loss: 0.0121 - acc: 0.9953\n",
      "Epoch 97/200\n",
      "100/100 [==============================] - 77s 771ms/step - loss: 0.0060 - acc: 0.9981\n",
      "Epoch 98/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0243 - acc: 0.9926\n",
      "Epoch 99/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0168 - acc: 0.9944\n",
      "Epoch 100/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0148 - acc: 0.9953\n",
      "Epoch 101/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0159 - acc: 0.9947\n",
      "Epoch 102/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0146 - acc: 0.9966\n",
      "Epoch 103/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0157 - acc: 0.9956\n",
      "Epoch 104/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0117 - acc: 0.9966\n",
      "Epoch 105/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0096 - acc: 0.9969\n",
      "Epoch 106/200\n",
      "100/100 [==============================] - 76s 760ms/step - loss: 0.0202 - acc: 0.9955\n",
      "Epoch 107/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0205 - acc: 0.9930\n",
      "Epoch 108/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.0134 - acc: 0.9956\n",
      "Epoch 109/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.0152 - acc: 0.9962\n",
      "Epoch 110/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0136 - acc: 0.9972\n",
      "Epoch 111/200\n",
      "100/100 [==============================] - 77s 765ms/step - loss: 0.0333 - acc: 0.9925\n",
      "Epoch 112/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0112 - acc: 0.9959\n",
      "Epoch 113/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0228 - acc: 0.9914\n",
      "Epoch 114/200\n",
      "100/100 [==============================] - 76s 758ms/step - loss: 0.0133 - acc: 0.9958\n",
      "Epoch 115/200\n",
      "100/100 [==============================] - 76s 763ms/step - loss: 0.0103 - acc: 0.9972\n",
      "Epoch 116/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0152 - acc: 0.9953\n",
      "Epoch 117/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.0113 - acc: 0.9962\n",
      "Epoch 118/200\n",
      "100/100 [==============================] - 77s 765ms/step - loss: 0.0107 - acc: 0.9978\n",
      "Epoch 119/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0147 - acc: 0.9950\n",
      "Epoch 120/200\n",
      "100/100 [==============================] - 77s 765ms/step - loss: 0.0132 - acc: 0.9966\n",
      "Epoch 121/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.0348 - acc: 0.9915\n",
      "Epoch 122/200\n",
      "100/100 [==============================] - 76s 760ms/step - loss: 0.0111 - acc: 0.9969\n",
      "Epoch 123/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0147 - acc: 0.9945\n",
      "Epoch 124/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.0201 - acc: 0.9941\n",
      "Epoch 125/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.0097 - acc: 0.9953\n",
      "Epoch 126/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0117 - acc: 0.9959\n",
      "Epoch 127/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.0161 - acc: 0.9944\n",
      "Epoch 128/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0087 - acc: 0.9969\n",
      "Epoch 129/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.0044 - acc: 0.9987\n",
      "Epoch 130/200\n",
      "100/100 [==============================] - 76s 761ms/step - loss: 0.0135 - acc: 0.9959\n",
      "Epoch 131/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0168 - acc: 0.9950\n",
      "Epoch 132/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0114 - acc: 0.9961\n",
      "Epoch 133/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0072 - acc: 0.9975\n",
      "Epoch 134/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0035 - acc: 0.9987\n",
      "Epoch 135/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.0071 - acc: 0.9984\n",
      "Epoch 136/200\n",
      "100/100 [==============================] - 77s 767ms/step - loss: 0.0126 - acc: 0.9961\n",
      "Epoch 137/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0047 - acc: 0.9984\n",
      "Epoch 138/200\n",
      "100/100 [==============================] - 76s 761ms/step - loss: 0.0223 - acc: 0.9956\n",
      "Epoch 139/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0114 - acc: 0.9969\n",
      "Epoch 140/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.0034 - acc: 0.9994\n",
      "Epoch 141/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0079 - acc: 0.9969\n",
      "Epoch 142/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0155 - acc: 0.9944\n",
      "Epoch 143/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0126 - acc: 0.9961\n",
      "Epoch 144/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0111 - acc: 0.9969\n",
      "Epoch 145/200\n",
      "100/100 [==============================] - 77s 770ms/step - loss: 0.0091 - acc: 0.9975\n",
      "Epoch 146/200\n",
      "100/100 [==============================] - 76s 761ms/step - loss: 0.0077 - acc: 0.9972\n",
      "Epoch 147/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0113 - acc: 0.9962\n",
      "Epoch 148/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0045 - acc: 0.9981\n",
      "Epoch 149/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0130 - acc: 0.9964\n",
      "Epoch 150/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.0101 - acc: 0.9969\n",
      "Epoch 151/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.0117 - acc: 0.9972\n",
      "Epoch 152/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.0082 - acc: 0.9966\n",
      "Epoch 153/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0046 - acc: 0.9987\n",
      "Epoch 154/200\n",
      "100/100 [==============================] - 76s 763ms/step - loss: 0.0087 - acc: 0.9969\n",
      "Epoch 155/200\n",
      "100/100 [==============================] - 77s 771ms/step - loss: 0.0105 - acc: 0.9966\n",
      "Epoch 156/200\n",
      "100/100 [==============================] - 77s 772ms/step - loss: 0.0069 - acc: 0.9978\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 77s 769ms/step - loss: 0.0141 - acc: 0.9970\n",
      "Epoch 158/200\n",
      "100/100 [==============================] - 77s 769ms/step - loss: 0.0060 - acc: 0.9972\n",
      "Epoch 159/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0070 - acc: 0.9972\n",
      "Epoch 160/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0090 - acc: 0.9972\n",
      "Epoch 161/200\n",
      "100/100 [==============================] - 77s 768ms/step - loss: 0.0065 - acc: 0.9975\n",
      "Epoch 162/200\n",
      "100/100 [==============================] - 76s 761ms/step - loss: 0.0098 - acc: 0.9972\n",
      "Epoch 163/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0117 - acc: 0.9953\n",
      "Epoch 164/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0074 - acc: 0.9978\n",
      "Epoch 165/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0027 - acc: 0.9981\n",
      "Epoch 166/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0042 - acc: 0.9984\n",
      "Epoch 167/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0092 - acc: 0.9972\n",
      "Epoch 168/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0108 - acc: 0.9955\n",
      "Epoch 169/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0039 - acc: 0.9984\n",
      "Epoch 170/200\n",
      "100/100 [==============================] - 76s 758ms/step - loss: 0.0192 - acc: 0.9948\n",
      "Epoch 171/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0137 - acc: 0.9959\n",
      "Epoch 172/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0086 - acc: 0.9975\n",
      "Epoch 173/200\n",
      "100/100 [==============================] - 76s 763ms/step - loss: 0.0143 - acc: 0.9961\n",
      "Epoch 174/200\n",
      "100/100 [==============================] - 76s 763ms/step - loss: 0.0063 - acc: 0.9978\n",
      "Epoch 175/200\n",
      "100/100 [==============================] - 76s 762ms/step - loss: 0.0075 - acc: 0.9969\n",
      "Epoch 176/200\n",
      "100/100 [==============================] - 76s 763ms/step - loss: 0.0070 - acc: 0.9978\n",
      "Epoch 177/200\n",
      "100/100 [==============================] - 76s 763ms/step - loss: 0.0047 - acc: 0.9987\n",
      "Epoch 178/200\n",
      "100/100 [==============================] - 76s 757ms/step - loss: 0.0049 - acc: 0.9978\n",
      "Epoch 179/200\n",
      "100/100 [==============================] - 76s 762ms/step - loss: 0.0065 - acc: 0.9984\n",
      "Epoch 180/200\n",
      "100/100 [==============================] - 76s 763ms/step - loss: 0.0059 - acc: 0.9981\n",
      "Epoch 181/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0040 - acc: 0.9984\n",
      "Epoch 182/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.0070 - acc: 0.9961\n",
      "Epoch 183/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.0046 - acc: 0.9984\n",
      "Epoch 184/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.0069 - acc: 0.9978\n",
      "Epoch 185/200\n",
      "100/100 [==============================] - 76s 763ms/step - loss: 0.0067 - acc: 0.9987\n",
      "Epoch 186/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.0047 - acc: 0.9978\n",
      "Epoch 187/200\n",
      "100/100 [==============================] - 76s 758ms/step - loss: 0.0136 - acc: 0.9970\n",
      "Epoch 188/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0024 - acc: 0.9991\n",
      "Epoch 189/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.0068 - acc: 0.9972\n",
      "Epoch 190/200\n",
      "100/100 [==============================] - 77s 765ms/step - loss: 0.0106 - acc: 0.9975\n",
      "Epoch 191/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0046 - acc: 0.9984\n",
      "Epoch 192/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.0139 - acc: 0.9958\n",
      "Epoch 193/200\n",
      "100/100 [==============================] - 76s 762ms/step - loss: 0.0081 - acc: 0.9972\n",
      "Epoch 194/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0110 - acc: 0.9978\n",
      "Epoch 195/200\n",
      "100/100 [==============================] - 76s 759ms/step - loss: 0.0075 - acc: 0.9981\n",
      "Epoch 196/200\n",
      "100/100 [==============================] - 77s 766ms/step - loss: 0.0029 - acc: 0.9987\n",
      "Epoch 197/200\n",
      "100/100 [==============================] - 77s 765ms/step - loss: 0.0035 - acc: 0.9981\n",
      "Epoch 198/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0045 - acc: 0.9984\n",
      "Epoch 199/200\n",
      "100/100 [==============================] - 76s 765ms/step - loss: 0.0023 - acc: 0.9997\n",
      "Epoch 200/200\n",
      "100/100 [==============================] - 76s 764ms/step - loss: 0.0015 - acc: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x212f0189978>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FREEZE_LAYERS=2\n",
    "# 以訓練好的 ResNet50 為基礎來建立模型，\n",
    "# 捨棄 ResNet50 頂層的 fully connected layers\n",
    "net = ResNet50(include_top=False, weights='imagenet', input_tensor=None,\n",
    "               input_shape=(IMAGE_SIZE[0],IMAGE_SIZE[1],3))\n",
    "x = net.output\n",
    "x = Flatten()(x)\n",
    "\n",
    "# 增加 DropOut layer\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# 增加 Dense layer，以 softmax 產生個類別的機率值\n",
    "output_layer = Dense(5, activation='softmax', name='softmax')(x)\n",
    "\n",
    "# 設定凍結與要進行訓練的網路層\n",
    "net_final = Model(inputs=net.input, outputs=output_layer)\n",
    "for layer in net_final.layers[:FREEZE_LAYERS]:\n",
    "    layer.trainable = False\n",
    "for layer in net_final.layers[FREEZE_LAYERS:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# 使用 Adam optimizer，以較低的 learning rate 進行 fine-tuning\n",
    "net_final.compile(optimizer=Adam(lr=1e-5),\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 輸出整個網路結構\n",
    "# print(net_final.summary())\n",
    "\n",
    "# 訓練模型\n",
    "net_final.fit_generator(train_batches,\n",
    "#                         steps_per_epoch = train_batches.samples // BATCH_SIZE,\n",
    "                        steps_per_epoch = 100,\n",
    "#                         validation_data = validation_batches,\n",
    "#                         validation_steps = validation_batches.samples // BATCH_SIZE,\n",
    "                        epochs = 200,\n",
    "                        callbacks=[cb_checkpointer],\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_final.save_weights('final_100batch_200epoch.h5')  # always save your weights after training or during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 1 classes.\n",
      "2000/2000 [==============================] - 40s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_path,\n",
    "                                                  target_size=(224, 224),\n",
    "                                                  shuffle = False,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  batch_size=1)\n",
    "\n",
    "filenames = test_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "\n",
    "predict = net_final.predict_generator(test_generator,steps = nb_samples, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def decode(datum):\n",
    "    return datum.idxmax(axis=1)\n",
    "pred = decode(pd.DataFrame(predict))\n",
    "ids = [re.findall('\\w+', i)[1] for i in test_generator.filenames]\n",
    "sub = pd.DataFrame({'id': ids, 'flower_class': list(pred)})\n",
    "sub.to_csv('final.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> baseline - 0.90400"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
