{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day065\n",
    "## Word2Vec - gensim 使用簡介\n",
    "### 什麼是Word2vec?\n",
    "- NLP 裡面，最低階組成的是詞語，詞語組成句子，句子再組成段落、篇章、文檔，其關係就如同是二元一次方程式。把x 看做一個句子裡的一個詞語，y 是這個詞語的上下文詞語，那麼這裡的f，便是NLP 中經常出現的『語言模型』（language model），這個模型的目的，就是判斷(x,y) 這個樣本，是否符合自然語言的法則，Word2vec 正是來源於這個思想。\n",
    "- word2vec 是在做一項翻譯的工作，把詞（word）轉換成電腦可以了解的模式（vector），利用vector 之間的距離關係來決定兩者的相依程度。\n",
    "- Word2Vec 是尋找單詞連續embedding 的技術。通過閱讀大量的文本學習，並記憶哪些單詞傾向於相似的語境。訓練足夠多的資料後，詞彙表中的每個單詞會生成一個 300 維的向量，由意思相近的單詞構成，也就是說他會把\n",
    "這個詞附近的相鄰詞考慮進來，並把相關、片斷的字聯想一起。\n",
    "\n",
    "### 使用技術：\n",
    "1. Continuous Bag Of Words（CBOW），此方法會利用上下文的詞來當作神經網路的輸入，最後預測這個目標的詞是什麼。\n",
    "2. Skip-Gram 演算法，剛好跟第一種演算法相反，他所輸入的是當前的詞來預測這一段的文章上下文的詞。\n",
    "\n",
    "\n",
    "下圖即為將句子表示為詞袋。上面為句子，下面為對應的表示[0, 0, 1, 1, 0, 1, 1, 1]，向量中的每個數位（索引）代表一個特定的單詞\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*j3HUg18QwjDJTJwW9ja5-Q.png\">\n",
    "\n",
    "- word2vec 使用 Cosine Similarity 來計算兩個詞的相似性，這是一個 -1 到 1 的數值，如果兩個詞完全一樣就是 1，像是「台灣」這個詞和他自⾝身的 Cosine Similarity 就是 1，而除了比較相似性以外， word2vec 還可類推這個概念，從下圖來看，我們可以看到國家的距離是彼此相近的，中國、俄國、日本．．．等，而相對應首都的距離也是相近的\n",
    "<img src=\"http://1.bp.blogspot.com/-Q7F8ulD6fC0/UgvnVCSGmXI/AAAAAAAAAbg/MCWLTYBufhs/s1600/image00.gif\" width=\"500\" height=\"500\">\n",
    "\n",
    "### How to 實現  “word2vec” 詞向量?\n",
    "- 一個詞的意涵跟他的左右鄰居很有關係，左右鄰居就如同英文克漏字或是中文的填空數獨，利用windows的概念可以藉由Windows size 的設定可以讓你知道找幾個鄰居"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 範例 - 使用GENSIM來了解word2vec\n",
    "其流程為: 取得語料 → 執行GENSIM → 訓練詞向量 → 詞向量實驗 → 取得兩個詞的相似度\n",
    "\n",
    "### 參數說明\n",
    "- sentences: 要訓練的句子集，沒有他就不用跑了\n",
    "- size: 這表示的是訓練出的詞向量會有幾維\n",
    "- alpha: 機器學習中的學習率，這東西會逐漸收斂到 min_alpha \n",
    "- sg: sg=1表示採用skip-gram，sg=0 表示採用cbow \n",
    "- window: 能往左往右看幾個字的意思\n",
    "- workers: 執行緒數目，若是運行過程中碰到記憶體不足的問題，可以把worker的值設置在 4 以下\n",
    "- min_count: 若這個詞出現的次數小於min_count，那他就不會被視為訓練對象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChihYing\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim, logging\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-10 13:38:18,778 : INFO : collecting all words and their counts\n",
      "2019-04-10 13:38:18,782 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-10 13:38:18,783 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2019-04-10 13:38:18,784 : INFO : Loading a fresh vocabulary\n",
      "2019-04-10 13:38:18,784 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2019-04-10 13:38:18,785 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2019-04-10 13:38:18,786 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2019-04-10 13:38:18,789 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2019-04-10 13:38:18,791 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2019-04-10 13:38:18,792 : INFO : estimated required memory for 3 words and 256 dimensions: 7644 bytes\n",
      "2019-04-10 13:38:18,793 : INFO : resetting layer weights\n",
      "2019-04-10 13:38:18,795 : INFO : training model with 4 workers on 3 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-04-10 13:38:18,803 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-10 13:38:18,805 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-10 13:38:18,807 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-10 13:38:18,808 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-10 13:38:18,809 : INFO : EPOCH - 1 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-10 13:38:18,818 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-10 13:38:18,819 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-10 13:38:18,820 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-10 13:38:18,820 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-10 13:38:18,821 : INFO : EPOCH - 2 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-10 13:38:18,832 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-10 13:38:18,832 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-10 13:38:18,833 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-10 13:38:18,833 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-10 13:38:18,834 : INFO : EPOCH - 3 : training on 4 raw words (1 effective words) took 0.0s, 218 effective words/s\n",
      "2019-04-10 13:38:18,841 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-10 13:38:18,844 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-10 13:38:18,846 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-10 13:38:18,846 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-10 13:38:18,847 : INFO : EPOCH - 4 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-10 13:38:18,856 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-10 13:38:18,857 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-10 13:38:18,858 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-10 13:38:18,859 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-10 13:38:18,860 : INFO : EPOCH - 5 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-10 13:38:18,862 : INFO : training on a 20 raw words (1 effective words) took 0.1s, 15 effective words/s\n",
      "2019-04-10 13:38:18,863 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)  \n",
    "  \n",
    "sentences = [['first', 'sentence'], ['second', 'sentence']]  \n",
    "# train word2vec on the two sentences  \n",
    "model = word2vec.Word2Vec(sentences, size=256, min_count=1, window=5, workers=4, sg=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=3, size=256, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChihYing\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12779287504951806"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('first','second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-10 13:38:18,916 : INFO : saving Word2Vec object under mymodel, separately None\n",
      "2019-04-10 13:38:18,917 : INFO : not storing attribute vectors_norm\n",
      "2019-04-10 13:38:18,919 : INFO : not storing attribute cum_table\n",
      "2019-04-10 13:38:18,931 : INFO : saved mymodel\n",
      "2019-04-10 13:38:18,933 : INFO : loading Word2Vec object from mymodel\n",
      "2019-04-10 13:38:18,936 : INFO : loading wv recursively from mymodel.wv.* with mmap=None\n",
      "2019-04-10 13:38:18,939 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-04-10 13:38:18,940 : INFO : loading vocabulary recursively from mymodel.vocabulary.* with mmap=None\n",
      "2019-04-10 13:38:18,941 : INFO : loading trainables recursively from mymodel.trainables.* with mmap=None\n",
      "2019-04-10 13:38:18,943 : INFO : setting ignored attribute cum_table to None\n",
      "2019-04-10 13:38:18,945 : INFO : loaded mymodel\n"
     ]
    }
   ],
   "source": [
    "model.save('mymodel')  \n",
    "new_model = gensim.models.Word2Vec.load('mymodel')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業:\n",
    "嘗試調整參數:  \n",
    "sg:sg=1表示採用skip-gram,sg=0 表示採用cbow  \n",
    "window:能往左往右看幾個字的意思 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-10 13:38:18,960 : INFO : collecting all words and their counts\n",
      "2019-04-10 13:38:18,963 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-10 13:38:18,964 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2019-04-10 13:38:18,965 : INFO : Loading a fresh vocabulary\n",
      "2019-04-10 13:38:18,967 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2019-04-10 13:38:18,969 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2019-04-10 13:38:18,971 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2019-04-10 13:38:18,971 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2019-04-10 13:38:18,972 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2019-04-10 13:38:18,974 : INFO : estimated required memory for 3 words and 256 dimensions: 7644 bytes\n",
      "2019-04-10 13:38:18,974 : INFO : resetting layer weights\n",
      "2019-04-10 13:38:18,975 : INFO : training model with 4 workers on 3 vocabulary and 256 features, using sg=1 hs=0 sample=0.001 negative=5 window=2\n",
      "2019-04-10 13:38:18,983 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-10 13:38:18,985 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-10 13:38:18,986 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-10 13:38:18,987 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-10 13:38:18,989 : INFO : EPOCH - 1 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-10 13:38:18,993 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-10 13:38:18,996 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-10 13:38:18,998 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-10 13:38:18,999 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-10 13:38:19,000 : INFO : EPOCH - 2 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-10 13:38:19,006 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-10 13:38:19,007 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-10 13:38:19,008 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-10 13:38:19,008 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-10 13:38:19,009 : INFO : EPOCH - 3 : training on 4 raw words (1 effective words) took 0.0s, 268 effective words/s\n",
      "2019-04-10 13:38:19,016 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-10 13:38:19,017 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-10 13:38:19,019 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-10 13:38:19,019 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-10 13:38:19,021 : INFO : EPOCH - 4 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-10 13:38:19,024 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-10 13:38:19,025 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-10 13:38:19,026 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-10 13:38:19,027 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-10 13:38:19,028 : INFO : EPOCH - 5 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-10 13:38:19,031 : INFO : training on a 20 raw words (1 effective words) took 0.1s, 18 effective words/s\n",
      "2019-04-10 13:38:19,032 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "sentences = [['I am a hero', 'sentence'], ['She is a teacher', 'sentence']] \n",
    "\n",
    "# train word2vec on the two sentences  \n",
    "model = word2vec.Word2Vec(sentences, size=256, min_count=1, window=2, workers=4, sg=1)\n",
    "# sg=0 表示COBW, sg=1 表示skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=3, size=256, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChihYing\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.016742332986403766"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model)\n",
    "model.similarity('I am a hero','She is a teacher')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
