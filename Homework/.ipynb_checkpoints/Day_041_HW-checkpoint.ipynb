{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day041\n",
    "## tree based model - 決策樹 (Decision Tree) 模型介紹\n",
    "<img src=\"https://i0.wp.com/dataaspirant.com/wp-content/uploads/2017/01/B03905_05_01-compressor.png?resize=1024%2C566\" width=\"500\" height=\"500\">\n",
    "\n",
    "- 透過一系列是非問題，把資料分類，最初的node稱為root，最下面沒有children的node稱為leaf。\n",
    "- 每個決策過程都可以視覺化，是個具有非常高解釋性的模型(讓人聽得懂)\n",
    "- 決策是從訓練資料中找出規則(supervised)，且讓每一次決策能使訊息增益 (Information Gain)最大化\n",
    "- 訊息增益越大代表切分後的兩群資料，群內相似程度越高 <br>\n",
    "   e.g. 使用健檢資料來預測性別，若使用頭髮長度超過 50 公分來切分，則切分後兩群資料很有可能一邊大部分是男生，一邊大部分是女生(相似程度高)，這樣頭髮長度就是個很好的 feature。\n",
    "\n",
    "### 訊息增益 (Information Gain)\n",
    "- 決策樹模型使用 features 切分資料，該選用哪個 feature 來切分是由訊息增益的大小決定的。希望切分後的資料相似程度很高，通常用吉尼係數來衡量相似程度。\n",
    "- 有兩種衡量相似程度的方法，其實兩者都差不多，但用[Gini計算比較簡單](https://datascience.stackexchange.com/questions/10228/gini-impurity-vs-entropy):\n",
    "\n",
    "    其中 $p_j, j=1, ...m$ 為在第 j 分類中資料佔全體的比例(機率分布)\n",
    "    1. Gini impurity: 在單個 $p_j = 1$ 且其他 $p_j = 0$ 時(也就是只有一種分類) 為 0，測量每個分類內的資料混合程度，越低越好<br>\n",
    "        $$Gini = 1 - \\sum_{j}{p_j^2}$$\n",
    "        <img src=\"https://cdn-images-1.medium.com/max/1000/1*ec2gh0K26Drvb1pyQk8jVg.png\" height=\"300\" width=\"500\">\n",
    "    2.  Information gain (Entropy): 只有一種分類時最小，所有 $p_j$ 皆相等時最大<br>\n",
    "        $$Entropy = - \\sum_{j}{p_jlog_2p_j}$$\n",
    "        \n",
    "    最後再以下列公式計算訊息增益\n",
    "    <img src=\"https://cdn-images-1.medium.com/max/1000/1*FjGUfcYt_Vyupv1KfNm_pA.png\">\n",
    "    \n",
    "### 決策樹的特徵重要性 (Feature importance)\n",
    "- 我們可以從構建樹的過程中，透過 feature 被用來切分的次數，來得知哪些features 是相對有用的\n",
    "- 所有 feature importance 的總和為 1\n",
    "- 實務上可以使用 feature importance 來了解模型如何進行分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "\n",
    "閱讀以下兩篇文獻，了解決策樹原理，並試著回答後續的問題\n",
    "- [決策樹 (Decision Tree) - 中文](https://medium.com/@yehjames/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda)\n",
    "- [how decision tree works - 英文](http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/)\n",
    "\n",
    "\n",
    "1. 在分類問題中，若沒有任何限制，決策樹有辦法在訓練時將 training loss 完全降成 0 嗎？\n",
    "> 可以，但這樣會造成over-fitting\n",
    "2. 決策樹只能用在分類問題嗎？還是可以用來解決回歸問題？\n",
    "> 可以解決回歸問題，使用information Gain作為標準時要求attributes為類別型，使用gini index則要求attributes為連續"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 補充資料 - [Let’s Write a Decision Tree Classifier from Scratch ](https://www.youtube.com/watch?v=LDRbO9a6XPU)\n",
    "\n",
    "\n",
    "原notebook在[這](https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb)，使用CART (Classification And Regression Tree)演算法來種樹"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
