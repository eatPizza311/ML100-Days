{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day045\n",
    "## tree based model - 梯度提升機 (Gradient Boosting Machine) 介紹\n",
    "- 隨機森林使用的集成方法稱為 Bagging (Bootstrap aggregating)，用抽樣的資料與 features 生成每一棵樹，最後再取平均\n",
    "- Boosting 則是另一種集成方法，希望能夠由後面生成的樹，來修正前面樹學不好的地方，GBM的修正方法為計算梯度，每次生成樹都是要修正前面樹預測的錯誤，並乘上 learning rate 讓後面的樹能有更多學習的空間\n",
    "\n",
    "### Bagging 與 Boosting 的差別\n",
    "- Bagging 是透過抽樣 (sampling)的方式來生成每一棵樹，樹與樹之間是獨立生成的\n",
    "- Boosting 是透過序列 (additive) 的方式來生成每一顆樹，每棵樹都會與前面的樹關聯，因為後面的樹要能夠修正\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*8T4HEjzHto_V8PrEFLkd9A.png\" width=600 height=500>\n",
    "\n",
    "圖片來自 [medium](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)\n",
    "\n",
    "本日作業請完整閱讀以下文獻即可\n",
    "\n",
    "[完整的 Ensemble 概念 by 李宏毅教授](https://www.youtube.com/watch?v=tH9FH1DH5n0)\n",
    "\n",
    "[深入了解 Gradient-boosting](https://explained.ai/gradient-boosting/index.html)\n",
    "\n",
    "\n",
    "你可能聽過 XGBoost/Light-GBM，這些都是資料科學競賽中最常用的機器學習模型，但其實這些演算法背後原理都是基於 Gradient-boosting 進而優化，強烈建議您對本日的課程與補充教材多花點時間閱讀與理解。\n",
    "\n",
    "\n",
    "[梯度提升機原理](https://ifun01.com/84A3FW7.html)\n",
    "\n",
    "[Kaggle 大師帶你了解梯度提升機原理](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)\n",
    "\n",
    "[XGboost 作者講解原理](https://www.youtube.com/watch?v=ufHo8vbk6g4)\n",
    "\n",
    "[XGBoost 數學原理 slides](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Gradient boosting: Distance to target](https://explained.ai/gradient-boosting/L2-loss.html)\n",
    "\n",
    "介紹最常見的GBM形式，其最佳化了MSE，又稱為$L_2$ loss or cost.\n",
    "\n",
    "GBM將幾個弱的model組合成一個強的model，而且每個額外的弱model都會降低整體的MSE。\n",
    "\n",
    "### Additive modeling\n",
    "可加模型是boosting的基礎，它的想法就是將很多簡單的項加起來以創造複雜的形式，在ML的世界中，這樣比一次找到一個完美的function來說還實際。\n",
    "\n",
    "考慮下圖做為目標函數\n",
    "<img src=\"https://explained.ai/gradient-boosting/images/L2-loss/L2-loss_additive_2.svg\" height=300 width=300>\n",
    "\n",
    "假設圖中的函數為幾個簡單的項組合而成，我們可以由簡單的函數重建它，並在每個項加入後都考慮現在的組合函數跟目標函數的差距，以決定下一個要加入的項為何。\n",
    "\n",
    "我們第一個近似可以是水平線 $y = 30$，因為y的截點在 $y(x=0) = 30$，但很顯然這個斜率不對，所以我們可以再加入穿過彎曲的目標函數的45度直線 $y = x$，而彎曲的部分其實來自於sine，所以我們加入後就能得到目標函數。\n",
    "<img src=\"https://explained.ai/gradient-boosting/images/L2-loss/L2-loss_additive_3.svg\" height=300 width=900>\n",
    "\n",
    "\n",
    "解析複雜函數的步驟就像寫程式一樣，要將其拆分成較小的部分各個擊破，以上面的例子就是把我們的目標函數 $y = F(x) = 30 + x + sin(x)$，拆成幾個函數的組合 $F(x) = f_1(x) + f_2(x) + f_3(x)$ 其中 $f_1(x) = 30$、$f_2(x) = x$、$f_3(x) = sin(x)$，更廣義的來說就是:\n",
    "\n",
    "$$F_M(x) = f_1(x) + ... + f_M(x) = \\sum_{i=1}^{M}{f_i(x)}$$\n",
    "\n",
    "\n",
    "### boosted regression\n",
    "Boosting是將多個簡單的模型組合成單個模型的策略，其中心想法為隨著簡單的模型的引入，整體模型會變成更強大預測器，其中簡單的模型被稱為weak models或weak learners。\n",
    "\n",
    "回歸(regression)預測的是數字，每一個observation都有一個特徵的向量(vector of features)，稱為$\\textbf{x}$，利用$\\textbf{x}$我們可以藉由 $(x_i, y_i)$ 學習，給定一個特徵 $\\textbf{x}$ 與純量目標值 $y$，我們可以把由weak model ${f_i(\\textbf{x})}$組合而成的模型寫為:\n",
    "$$ \\hat{y} = \\sum_{i=1}^{M}{f_i(\\textbf{x})} $$\n",
    "實務上${f_i(\\textbf{x})}$ 不只是函數，也可以是k-nearest-neighbors or regression trees。\n",
    "\n",
    "Boosting的${f_i(\\textbf{x})}$並不是獨立且同時創造的，而是一個接一個創造，並且被選擇來改善整體模型表現，直到表現夠好了或再加入也不會改善為止。\n",
    "\n",
    "${f_i(\\textbf{x})}$的選擇策略是貪心的，也就是不會改變先前已選擇的，只會一直加上去，所以boosting models常常被寫成recursive formulation:\n",
    "\n",
    "$ F_m(\\textbf{x}) = F_{m-1}(\\textbf{x}) + F_m(\\textbf{x})$\n",
    "\n",
    "代表我們都會用${f_i(\\textbf{x})}$來改善先前的模型以得到下一個模型。\n",
    "\n",
    "\n",
    "\n",
    "### The intuition behind gradient boosting\n",
    "為了創造boosted regression model，我們由一個廢廢的模型$f_0(\\textbf{x})$開始，接著慢慢加入 $\\Delta_m(\\textbf{x})$ 微調整體模型$F_M(\\textbf{x})$\n",
    "$$ \\hat{y} = f_0(\\textbf{x}) + \\Delta_1(\\textbf{x}) + \\Delta_2(\\textbf{x}) + ... + \\Delta_M(\\textbf{x}) = f_0(\\textbf{x}) + \\sum_{i=1}^{M}{\\Delta_i(\\textbf{x})} = F_M(\\textbf{x}) $$\n",
    "\n",
    "可以把boosting 方法想為一個高爾夫球手打球的樣子，如下圖\n",
    "<img src=\"https://explained.ai/gradient-boosting/images/golf-dir-vector.png\" height=300 width=300>\n",
    "\n",
    "在打完第一次後，高爾夫球手藉由計算第一次近似與目標值的差 $y - f_0(\\textbf{x})$，這個差被稱為 residual or residual vector，在gradient boosting可以把它想成由現在的預測 $\\hat{y}$指向目標值 $y$的向量。\n",
    "\n",
    "使用residual vector微調，代表利用$y - F_{m-1}(\\textbf{x})$來訓練$\\Delta_m(\\textbf{x})$，就像所有的ML模型一樣，$\\Delta_m(\\textbf{x})$並不會有完美的 recall and precision，因此我們應該預期$\\Delta_m(\\textbf{x})$給出noisy prediction，而不是完美的$y - F_{m-1}(\\textbf{x})$，下表即是範例\n",
    "<img src=\"https://explained.ai/gradient-boosting/images/latex-0936C06C9B1327717E490FE17E434573.svg\" height=300 width=500>\n",
    "\n",
    "\n",
    "GBM也支持learning rate $\\eta$，以減少overfitting的機會。\n",
    "\n",
    "高爾夫球的例子就是boosting for regression的直覺理解(至少對單個observation的狀況是如此)，另外有幾點要注意\n",
    "\n",
    "1. weak models不只學習direction vectors的純量，還有其方向資訊。\n",
    "\n",
    "2. 初始模型$f_0(\\textbf{x})$嘗試藉由給定的$\\textbf{x}$學習目標值y，但$\\Delta_m(\\textbf{x})$嘗試由$\\textbf{x}$學習方向向量。\n",
    "\n",
    "3. 所有的weak models $f_0(\\textbf{x})$ 與 $\\Delta_m(\\textbf{x})$，訓練用的方向向量都是原來的特徵的向量$\\textbf{x}$的函數。\n",
    "\n",
    "4. 兩個常用的方向向量為residual: $y - F_{m-1}(\\textbf{x})$ 和 sign: $sign(y - F_{m-1}(\\textbf{x}))$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
